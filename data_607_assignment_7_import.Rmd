---
title: "data_607_assignment_7"
author: "Maxfield Raynolds"
date: "2025-03-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages}
library(tidyverse)
```
## Load the Data

The below code block loads A csv file into the r workspace as a dataframe. Since there are only 7 column headers in the first row and there are 8 items separated by commas in all subsequent rows loading the dataframe causes an error. It loads the final two features of the csv to a single column.

```{r load csv}
cuny_mart_raw <- read_csv("https://raw.githubusercontent.com/mraynolds/data_607/refs/heads/main/cuny_mart.csv", show_col_types = FALSE)
problems(cuny_mart_raw)
head(cuny_mart_raw)
```

The following code separates out the data in the Variation Details into separate columns and then separating the key and item pairings. Finally it pivots the data wider so that each variation has its own column.

```{r separate variation details columns}
cuny_mart <- cuny_mart_raw |> 
  separate_wider_delim(`Variation Details`, delim = ", ", names = c("variation_details", "variation_details_2")) |> 
  separate_wider_delim(variation_details, delim = ": ", names = c("variation", "variation_detail")) |> 
  separate_wider_delim(variation_details_2, delim = ": ", names = c("variation_2", "variation_detail_2")) |> 
  pivot_wider(names_from = variation, values_from = variation_detail) |> 
  pivot_wider(names_from = variation_2, values_from = variation_detail_2, names_repair = "unique")

head(cuny_mart)
```
The following code cleans up the original data and renames columns.

```{r clean up the table}
cuny_mart <- cuny_mart |> 
  unite(color, c(Color...7, Color...16), na.rm = TRUE) |> 
  unite(size, c(Size...10, Size...14), na.rm = TRUE) |> 
  rename(
    category = Category,
    item_name = `Item Name`,
    item_id = `Item ID`,
    brand = Brand,
    price = Price,
    variation_id = `Variation ID`,
    type = Type,
    format = Format,
    material = Material,
    storage = Storage,
    capactiy = Capacity,
    language = Language
  )

head(cuny_mart)
```

## Parquet

The following code loads packages to use for parquet.

```{r load packages for parquet}
library(arrow)
library(dplyr, warn.conflicts = FALSE)
```


The following code block exports the data from the csv to parquet format. Parquet has a significant advantage over other methods as it stores data in smaller file sizes than other formats. It stores data in columns and allows for larger files to be stored and loaded faster. The drawback is the format is no longer human readable. It is also not efficient for small datasets.

```{r export to parquet}
pqpath <- "assignment_7"
write_dataset(cuny_mart, path = pqpath, format = "parquet")
```

The following code displays the first 100 bit of the raw parquet file. It also reads the file directly from github.

```{r display parquet code}
readBin("https://raw.githubusercontent.com/mraynolds/data_607/main/part-0.parquet", what = "raw", n = 100)
read_parquet("https://raw.githubusercontent.com/mraynolds/data_607/main/part-0.parquet", as_data_frame = TRUE)
```



The following code imports a parquet file into the r workspace and collects it for use in a dataframe.

```{r import parquet to r}
cuny_mart_parquet <- read_parquet("https://raw.githubusercontent.com/mraynolds/data_607/main/part-0.parquet", as_data_frame = TRUE)

head(cuny_mart_parquet)
```

A small bit of analysis: The following code displays the average price by category from the imported dataset.

```{r average price}
cuny_mart_parquet |> group_by(category) |> summarize(
  price = mean(price)
)
```


## JSON

```{r load packages for json}
library(jsonlite)
```

The following code exports the original dataframe to JSON format and displays the raw JSON file. 

```{r export to json}
cuny_mart_json_export <- toJSON(cuny_mart)
write(cuny_mart_json_export, "cuny_mart_json_export.json")
print(cuny_mart_json_export)
```



The following code imports the data from file of the data written in JSON. Javascript object notation (JSON) is frequently the format for API returns and is easily written and read by machines but is also relatively easy to read for humans. It also allows for a hierarchical structure which may be useful but can also complicate importing the data. Finally, it is widely supported.

```{r import the data from JSON}
cuny_mart_json <- fromJSON("https://raw.githubusercontent.com/mraynolds/data_607/refs/heads/main/cuny_mart_json_export.json")

head(cuny_mart_json)
```

A small bit of analysis: The following code counts the colors of the products from the imported json dataset.

```{r filter color}
cuny_mart_json |> filter(color != "") |> count(color)
```


## HTML
```{r load packages for html}
library(rvest)
library(htmlTable)
```

The following code exports the data from the prepared dataframe to an html file and displays the raw html file.

```{r export to html}
cuny_mart_html_export <- htmlTable(cuny_mart)
write(cuny_mart_html_export, "cuny_mart_html_export.html")
cat(cuny_mart_html_export)
```



The following code imports the data from a file of the data written in html. Hypertext Markup Language or HTML is used widely as a markup language, commonly in web pages. It is common and so manipulating data in html is a useful way of accessing data presented on webpages. The html from a website can then be scraped and then transformed. It is also relatively easy for humans to read basic html. However, it is not optimized for storing or communicating large amounts of data.

```{r import from html}
cuny_mart_html <- read_html("https://raw.githubusercontent.com/mraynolds/data_607/refs/heads/main/cuny_mart_html_export.html", )
cuny_mart_html <- cuny_mart_html |> html_element("table")|> html_table() |> select(category:language)

head(cuny_mart_html)
```

A small bit of analysis: The following code counts the number items in each category.

```{r count of items by category}
cuny_mart_html |> count(category)
```


## XML

The following code loads packages for use with xml.

```{r load packages for xml}
library(XML)
library(xml2)
library(methods)
library(xmlconvert)
```

The following code imports an xml file and displays the raw XML data. XML is Extensible Markup Language and is useful for carrying and storing data. It allows customization through tags and is hierarchical. However, it tends to have larger file sizes and slower queries so it is not useful for big data. 

```{r import an xml file}
cuny_mart_xml <- read_xml("https://raw.githubusercontent.com/mraynolds/data_607/refs/heads/main/cuny_mart_xml.xml")
print(cuny_mart_xml)
```
The following code block parses the XML and creates a dataframe from it.

```{r parse the xml}
cuny_mart_xml <- xmlParse(cuny_mart_xml)
cuny_mart_xml <- xmlToDataFrame(cuny_mart_xml)

head(cuny_mart_xml)
```

The following code exports a dataframe to xml.

```{r export to xml}
write_xml(df_to_xml(cuny_mart), "cuny_mart_xml.xml")
```

A small bit of analysis: The following code shows the dollars per gigabyte for the electronic items.

```{r dollars per gb}
cuny_mart_xml |> mutate(
  storage = str_remove_all(storage, "\\D*"),
  dollar_per_gb = (as.numeric(price)/as.numeric(storage))
) |> filter(dollar_per_gb != "NA") |> select(item_name, brand, price, 
  dollar_per_gb
)
```

